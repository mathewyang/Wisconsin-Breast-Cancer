---
title: "Logistic regression"
author: "Mathew Yang"
output: html_document
---
I will be running a logistic regression model on this dataset to classify the tumor cells. 
```{r}
library(caret)
library(pROC)
#Load data
wdbc = read.csv("data/wdbc.data", header= FALSE)
```
#Principal Component Analysis

```{r}
#PCA
pca.model = prcomp(wdbc[,3:32])
summary(pca.model)
biplot(pca.model, scale = TRUE)                
```
PCA reveals that there are 2 cell features that contribute heavily to the dataset's variance: V6 (mean cell area) and V26 (max cell area). The first 2 principal components (PC) account for over 99% of the data variance, so we will opt to use just these two PCs.

#Logistic Regression Model

Here we will train and test on the same dataset. This is inadvisable but I'd like to compare the results to a cross-validated model.
```{r}
#Logistic regression on means.
#Train and test on same dataset.
cancer.fit = glm(V2 ~ pca.model$x[,1] + pca.model$x[,2], data=wdbc, family= binomial)
summary(cancer.fit)
```

```{r}
#Gives probabilities that the observation is a malignant tumor. Probability close to 1 is malignant, 0 is benign.
cancer.probs = predict(cancer.fit, type = "response")

#Create a vector of 569 Benign elements
cancer.pred=rep("B",569)

#Transform all the ones greate than 0.5 to Malignant.
cancer.pred[cancer.probs>0.5]="M"
```
Confusion Matrix
```{r}
table(cancer.pred, wdbc$V2)
```

Training error rate is 1 - (345+186)/569 = 0.067, which means that the model was capable of correctly guessing M or B almost 93.3% of the time. Next I will retest via crossvalidation or 10-fold LOOCV (leave-one-out cross-validation).

```{r}
#10-fold parameters
ctrl = trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)

#Perform logistic regression on 10-fold cross-validated data
mod_fit = train(V2~ ., 
                 data=wdbc, 
                 method="glm",
                 family = "binomial",
                 trControl = ctrl)
summary(mod_fit)
```

```{r}
pred = predict(mod_fit, type = "prob")
actual = as.numeric(wdbc$V2)-1
roc.log = roc(actual, pred$B)
```

An ROC curve is commonly used to analyze and compare the performance of models. The area under the ROC curve (AUC) provides a quantitative assessment of ???
```{r}
# Plot ROC
plot.roc(roc.log, xlab = "False Positive Rate", ylab="True Positive Rate", main="ROC Curve", print.auc=TRUE)
#Get AUC
auc(roc.log)
```

